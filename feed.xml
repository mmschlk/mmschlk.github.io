<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mmschlk.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mmschlk.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-05T14:59:59+00:00</updated><id>https://mmschlk.github.io/feed.xml</id><title type="html">blank</title><subtitle>Maximilian&apos;s personal website. </subtitle><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://mmschlk.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://mmschlk.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://mmschlk.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">What Are Shapley Interactions, and Why Should You Care?</title><link href="https://mmschlk.github.io/blog/2015/shapley-interactions/" rel="alternate" type="text/html" title="What Are Shapley Interactions, and Why Should You Care?"/><published>2015-12-05T12:00:00+00:00</published><updated>2015-12-05T12:00:00+00:00</updated><id>https://mmschlk.github.io/blog/2015/shapley-interactions</id><content type="html" xml:base="https://mmschlk.github.io/blog/2015/shapley-interactions/"><![CDATA[<h1 id="what-are-shapley-interactions-and-why-should-you-care">What Are Shapley Interactions, and Why Should You Care?</h1> <p>Shapley values are the go-to method for explainable AI because they are easy to interpret and theoretically well-founded. However, they struggle to capture the interplay between features. About two years ago, we attended a talk that introduced the concept of <strong>Shapley interactions</strong>, and we quickly realized that Shapley interactions were exactly the solution to our problem! Unfortunately, the presenter also mentioned that calculating Shapley interaction values is extremely difficult, if not impossible. We started experimenting a bit and soon came up with a preliminary approach to compute Shapley interactions for any machine learning model. After receiving positive feedback and growing interest from the community, we realized that this research gap was a perfect opportunity to pursue our own interests and create impact for others. That’s when our <a href="https://github.com/mmschlk/shapiq"><strong>shapiq</strong></a> project was born. Here, we want to tell you <em>what Shapley interactions are and why you should care</em>.</p> <h2 id="what-are-interactions">What Are Interactions?</h2> <p>Imagine you’re working with the California housing dataset and we want to predict house prices. Features like location, median income, and year built all play a role. Location, for example, is represented by longitude (west-east) and latitude (north-south). But here’s the twist: looking at longitude or latitude individually doesn’t tell the whole story. It’s their combination—pinpointing the exact location—that influences house prices.</p> <p>Take two houses (with otherwise similar properties) near the ocean:</p> <ul> <li>Close to San Francisco: $454,000.</li> <li>Far from major hotspots: $210,000.</li> </ul> <p>This difference isn’t explained by longitude or latitude alone but by their interaction. Interactions capture the idea that one feature’s influence (longitude) depends on the value of another (latitude).</p> <h2 id="limitations-of-shapley-values">Limitations of Shapley Values</h2> <p>Shapley values are popular because they distribute the effects of features fairly. For example, if you explain the above-mentioned house in San Francisco with a predicted house price of 454,000$, Shapley values show that both longitude and latitude positively contribute (see image below). But there’s a problem: Shapley values merge individual effects and interaction effects into a single number. This means:</p> <ul> <li>We can’t tell how much of a feature’s influence is individual versus interactive.</li> <li>We don’t know which other features it interacts with.</li> </ul> <p><img src="/assets/img/blog_post_shapley_interaction_1.png" alt="shapiq logo" width="400"/></p> <h2 id="enter-shapley-interactions">Enter Shapley Interactions</h2> <p>Shapley interactions enhance the traditional Shapley value approach by breaking down the effects of features into <strong>individual contributions</strong> and <strong>interactions between features</strong>. Instead of providing a single value per feature, Shapley interactions distribute the prediction’s influence across both individual features and groups of interacting features. So for each combination of features we, potentially, can get a value of an interaction.</p> <p>Let’s return to our <strong>California housing</strong> example, the decomposition up to order 2 compared to order 1 (Shapley value) shows that:</p> <ul> <li>Longitude has a high individual contribution, showing the importance of proximity to the ocean.</li> <li>Latitude, however, has little individual impact, with its contribution coming entirely from its interaction with longitude (in the image below latitude’s contribution vanishes but the interaction between longitude and latitude appears).</li> <li>This interaction captures how the combination of longitude and latitude identifies high-value locations, like San Francisco.</li> </ul> <p>By predefining a <strong>maximum interaction order,</strong> you can control how detailed the analysis gets. Setting it to second-order interactions, for instance, allows you to explore how pairs of features interact (like longitude and latitude) while still capturing individual effects. This provides a richer understanding of how features influence predictions—whether independently or through their interplay.</p> <p><img src="/assets/img/blog_post_shapley_interaction_2.png" alt="shapiq logo" width="400"/></p> <p>Hence, Shapley interactions give you more granular insights into the relationships driving model predictions, helping <strong>uncover synergies or redundancies that traditional Shapley values can’t</strong>. One of the main takeaways here is that we can interpret Shapley interactions like you are used to with SHAP, while providing more information. This level of detail, however, can come with a cost.</p> <h2 id="balancing-insights-and-complexity">Balancing Insights and Complexity</h2> <p>The more interactions we analyze (higher-order decompositions), the more insights we uncover. <strong>But there’s a tradeoff:</strong> explanations become more complex and harder to interpret (because we have way more values/interactions to analyze). Striking the right balance between depth and simplicity depends on your goals.</p> <h2 id="computing-shapley-interactions-in-python-with-shapiq">Computing Shapley Interactions in Python with shapiq</h2> <p>If you’re curious about how to compute these decompositions or visualize them, check out our <strong><a href="https://github.com/mmschlk/shapiq">shapiq package</a></strong> for an easy way to explore Shapley interactions and uncover new insights! Currently, we offer a range of model-agnostic and model-specific explainers and computation methods, which you can use to calculate Shapley interactions and Shapley values for all data types and model kinds. Check out the tutorial notebooks on how to use it for your task.</p> <p><img src="/assets/img/blog_post_shapley_interaction_3.png" alt="shapiq logo" width="400"/></p>]]></content><author><name></name></author><category term="shapley-interactions"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[A gentle introduction to Shapley interactions and why they matter.]]></summary></entry></feed>